---
title: "Lasso estimation in multiple linear regression"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
date: "03 de marzo de 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Lasso for the Boston Housing data 
The Boston House-price dataset concerns housing values in 506 suburbs of Boston corresponding to year 1978. They are available here:
   https://archive.ics.uci.edu/ml/datasets/Housing

The Boston House-price corrected dataset (available in boston.Rdata) con- tains the same data (with some corrections) and it also includes the UTM coor- dinates of the geographical centers of each neighborhood.

### 1.1 Lasso estimation using the package 'glmnet'
After loading the right package, the response and explanatory variables from the Boston Housing data are set.
```{r}
#install.packages("glmnet")
library(glmnet)
library(Matrix)

boston <- load("boston.Rdata")

response <- "CMEDV"

explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")

# glmnet cannot handle factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)
```

Fitting the Lasso Regression model.
```{r}
lasso_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = FALSE,
                intercept = FALSE)
lasso_boston$beta
plot(lasso_boston)
#text(locator(), labels = c("TAX", "B"))
# names of the betas missing in the plot

cv_lasso_boston <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = FALSE,
                intercept = FALSE)
cv_lasso_boston
plot(cv_lasso_boston)
```
The Lasso Estimation does variable selection automatically. This results in a model with two non-zero explanatory variables. The variables affecting the model are the full-value property tax rates ('TAX') and the proportion of blacks by town ('B'). 




### 1.2 Ridge Regression model using glmnet
Task description: 

Use glmnet to fit the previous model using ridge regression. Compare the 10-fold cross validation results from function cv.glmnet with those you obtained in the previous practice with your own functions.

```{r}
ridge_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = FALSE,
                intercept = FALSE)
ridge_boston$beta 
plot(ridge_boston)
```

The ridge regression of boston data using cv.glmnet
```{r}
cv_ridge_boston <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = FALSE,
                intercept = FALSE)
cv_ridge_boston$glmnet.fit$beta
```

The function that we have used in the other practice
```{r, warning=FALSE}
PMSE_k_fold <- function(X_t, Y_t, lambda.v, k=10){

  n_X_t <- dim(X_t)[1]
  p_X_t <- dim(X_t)[2] #length of columns
  n_subset <- as.integer((n_X_t)/k)+ 1
  group <- rep(seq(1,k), times = n_subset)
  group <- group[1:n_X_t]
  group_random <- sample(group)
  X_t_group <- cbind(X_t, group_random)
  Y_t_group <- cbind(Y_t, group_random)

#now we can start:

  PMSE <- list()

  for (la in 1:n_lambdas){

    lambda <- lambda.v[la]
    
    y_hat <- list()
    beta <- list() 
    h <- list()
    y <- list()

    for (l in 1:k){

      new_X_t_val <- subset(X_t_group, group_random==l)[ ,1:p_X_t]
      new_X_t_test <- subset(X_t_group, group_random!=l)[ ,1:p_X_t]

      new_Y_t_val <- subset.matrix(Y_t_group, group_random==l)[,1]
      new_Y_t_test <- subset.matrix(Y_t_group, group_random!=l)[,1]

      p_new <- dim(new_X_t_test)[2]
      p_new_v <- dim(new_X_t_val)[2]

      beta[[l]] <- solve(t(new_X_t_test)%*%new_X_t_test + lambda*diag(1,p_new))%*% t(new_X_t_test)%*%(new_Y_t_test)

      H_val <- new_X_t_val%*%solve(t(new_X_t_val)%*%new_X_t_val + (lambda+1e-13)*diag(1,p_new_v))%*% t(new_X_t_val) 

      # singular matrix for lambda = 0 -> trick: add a very small number 

      y_hat[[l]] <-  (new_X_t_val)%*%beta[[l]]
      h[[l]] <- diag(H_val)
      y[[l]] <- new_Y_t_val 

      
  }

  y_hat <- c(do.call(rbind, y_hat))
  beta <- c(do.call(cbind, beta))
  h <- c(do.call(rbind, h))
  y <- c(do.call(rbind, y))

  PMSE[[la]] <- 1/n_X_t * sum(((y-y_hat)/(1-h))^2)

  }

return(PMSE)

}


lambda.max = 2e8
n_lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1
```

```{r, warning=FALSE}
cv_ridge_boston
plot(cv_ridge_boston)


PMSE_val_10 <- PMSE_k_fold(X_t = as.matrix(boston.c[, explanatory]), Y_t = boston.c$CMEDV, lambda.v = lambda.v, k = 10)
plot(log(lambda.v[-1]+1), PMSE_val_10[-1], ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda+1)", main = "10-fold of Validation set")
#I am not sure if we have to keep or skip the +1 of this plot in the log(lambda)

```


## 2. A regression model with $p >> n$
Reading in the data.
```{r}
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)
death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
colnames(expr) <- paste("V", 1:nrow(express), sep = "")
```


### 2.1 Lasso estimation using glmnet for regressing 'log.surv' against 'expr'

Glmnet and cv.glmnet are used to obtain the lasso regressino for $log(surv)$ against $express$.

```{r}
lasso_surv <- glmnet(x = expr, y = log.surv,
                     alpha = 1)

cv_lasso_surv <- cv.glmnet(x = expr, y = log.surv,
                     alpha = 1)

# Number of non-zero coefficients
print("Number of non-zero coeff")
length(rownames(coef(cv_lasso_surv, s = "lambda.min"))[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0])

# Plot two graphics
par(mfrow=c(2,1))
plot(cv_lasso_surv)
plot(lasso_surv,xvar="lambda")
abline(v=log(cv_lasso_surv$lambda.min),col=2,lty=2)
abline(v=log(cv_lasso_surv$lambda.1se),col=2,lty=2)
par(mfrow=c(1,1))
```

There are 4 non zero coefficiants using Lasso regreassion. As one can see from the MSE plot this corresponds well with what looks like the point with the lowest MSE. From the coefficient plot one can also see that min lambda results in a vertical line crossing 4 betapaths which corresponds to 4 beta values unequal to zero. 

### 2.2 Computation of the responding fitted values
Task description: 

Compute the fitted values with the Lasso estimated model (you can use predict). Plot the observed values for the response variable against the Lasso fitted values.

```{r}
predict_lasso <- predict(lasso_surv,
                  newx = expr,
                  s = cv_lasso_surv$lambda.min)
plot( log.surv, predict_lasso)
abline(a=0, b=1, col = 2)
```

As one can see from the plot the real values are on a larger scale than the predicted values. This could mean that this is not the best way to modle this type of data. 


### 2.3 OLS regression model for 'log.surv' against 'expr'

Now we will fitt and OLS model with the responsvariables given by the non-zero coefficiants in the Lasso regreassion. 

```{r}
coeff_lasso <- rownames(coef(cv_lasso_surv, s = "lambda.min"))[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0]

coeff_lasso


lm_surv <- lm(log.surv ~ expr[, coeff_lasso[-1]])


plot(lm_surv$fitted.values, log.surv)
abline(a = 0, b = 1, col = 2)
```
As one can clearly see this gives a much better prediction for our data. The scales are more similar and the data is quite evenly distributet around the line $x=y$. 

### 2.4 Comparison of Lasso and OLS Regression
Task description: 

Compare the OLS and Lasso fitted values. Do a plot for that.

```{r, eval=FALSE}
print("Coefficiants Lasso regression")
coef(cv_lasso_surv, s = "lambda.min")[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0]
print("Coefficiants OLS")
lm_surv$coefficients

plot(lm_surv$fitted.values, predict_lasso)
abline(a = 0, b = 1, col = 2)
```

The OLS coefficiants are a lot larger than the Lasso regression coefficiants, this can also be seen in the plot where one can observe that the OLS fittes values are on a much larger scale than the Lasso fitted values. 

