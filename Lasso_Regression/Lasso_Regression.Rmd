---
title: "Lasso estimation in multiple linear regression"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
date: "03 de marzo de 2020"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Lasso for the Boston Housing data 
The Boston House-price dataset concerns housing values in 506 suburbs of Boston corresponding to year 1978. They are available here:
   https://archive.ics.uci.edu/ml/datasets/Housing

The Boston House-price corrected dataset (available in boston.Rdata) con- tains the same data (with some corrections) and it also includes the UTM coor- dinates of the geographical centers of each neighborhood.

### 1.1 Lasso estimation using the package 'glmnet'
After loading the right package, the response and explanatory variables from the Boston Housing data are set.

```{r, warning = FALSE}
#install.packages("glmnet")
library(glmnet)
library(Matrix)

boston <- load("boston.Rdata")

response <- "CMEDV"

explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")

# glmnet cannot handle factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)
```

Fitting the Lasso Regression model. (Correcting for previous mistakes standardize ans intercept is now set to TRUE)

```{r}
lasso_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = TRUE,
                intercept = TRUE)
lasso_boston$beta

cv_lasso_boston <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = TRUE,
                intercept = TRUE)
cv_lasso_boston
plot(cv_lasso_boston)

library(plotmo) #plot glmnet with coefficient names
plot_glmnet(lasso_boston, s = cv_lasso_boston$lambda.min)
```

The resulting  model has around $11$ non-zero explanatory variables and a mean square error around $23$ (changes a bit from run to run). 

### 1.2 Ridge Regression model using glmnet
We fit the dataset using glmnet and ridge regression and plot the results. 
Task description: 

```{r}
ridge_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = TRUE,
                intercept = TRUE)
#ridge_boston$beta 
```

The ridge regression of boston data using cv.glmnet.

```{r}
cv_ridge_boston <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = TRUE,
                intercept = TRUE)
cv_ridge_boston
plot(cv_ridge_boston)
plot_glmnet(ridge_boston, s = cv_ridge_boston$lambda.min)
```

The rigde regression has a few more non-zero expenatory variables than Lasso. A lot of them are very close to zero which is also the case in the Lasso regression. The same parameters are also the ones who are most clearly not equal to zero.

The 10-fold-function corrected.

```{r, warning=FALSE}
# MSE function to be used in MSPE
MSE <- function(Y_val, pred) {
  return(mean((Y_val - pred)^2))
}

MSPE<- function(X_t, Y_t, lambda.v, k_number){
  n_X_t <- dim(X_t)[1]
  shuffle_index <- sample(1:n_X_t, n_X_t, replace = FALSE)
  max_in_group <- n_X_t %/% k_number
  group_indexes <- split(shuffle_index, ceiling(seq(1:n_X_t) / max_in_group))
  group_indexes[[k_number]] <- c(group_indexes[[k_number]], group_indexes[[k_number + 1]])
  group_indexes[(k_number + 1)] <- NULL
  
  MSPE_groups <- data.frame(matrix(ncol = k_number, nrow = length(lambda.v)))
  
  n_lambdas <- length(lambda.v)
  
  for (k in 1:k_number){
    test <- group_indexes[[k]]
    train <- shuffle_index[!(shuffle_index %in% test)]
    
    #to meet assumption that the predictor variables have zero mean and unit variance 
    #and the response variables have zero mean

    Y<- scale(as.matrix(Y_t[train]), center = TRUE, scale = FALSE)
    X<- scale(as.matrix(X_t[train, ]), center = TRUE, scale = TRUE)
    
    #to be used to scale the validation set.
    center_X <- colMeans(as.matrix(X_t), na.rm = TRUE)
    center_Y <- colMeans(as.matrix(Y_t), na.rm = TRUE)
    scale_X <- sqrt(diag(cov(X_t)))
    
    n <- dim(X)[1]
    p <- dim(X)[2]
    
    XtX <- t(X) %*% X
    beta.path <- matrix(0, nrow = n_lambdas, ncol = p)
    diag.H.lambda <- matrix(0, nrow = n_lambdas, ncol = n)
    
    for (l in 1:n_lambdas) {
      lambda <- lambda.v[l] 
      H <- t(solve(XtX + lambda*diag(1, p)))%*% t(X) 
      beta.path[l, ] <- (H%*%Y)

    }
    
    Y_val <- scale(as.matrix(Y_t[test]), center = center_Y, scale = FALSE)
    X_val <- scale(as.matrix(X_t[test, ]), center = center_X, scale = scale_X)
    
    pred <- X_val %*% t(beta.path)
    MSPE_groups[, k] <- apply(pred, 2, MSE, Y_val = Y_val) # aply MSE to the columns of predict

  }
  MSPE_lambda <- rowMeans(MSPE_groups, na.rm = TRUE)
  
  lambda_opt <- lambda.v[which.min(MSPE_lambda)]
  
  return (list(
    MSPE.all = MSPE_lambda,
    lambda.opt = lambda_opt,
    MSPE.opt = min(MSPE_lambda),
    coeff.lambda = beta.path))
}


lambda.max = 1e9
n_lambdas <- 25
lambda.v <- exp(seq(0, log(lambda.max+1), length = n_lambdas)) - 1
```

Comparing the ridge regression using R cv_glmnet and our own function.

```{r, warning=FALSE}
library(ggplot2)

plot(cv_ridge_boston)

MSPE_val_5 <- MSPE(X_t = as.matrix(boston.c[, explanatory]), Y_t = boston.c$CMEDV, lambda.v = lambda.v, k = 10)

MSPE_values <- MSPE_val_5$MSPE.all
df <- data.frame(log(lambda.v), MSPE_values )
g <- ggplot( data = df, aes(log(lambda.v), MSPE_values)) + geom_point() + geom_vline(xintercept = log(MSPE_val_5$lambda.opt)) + ggtitle("10-fold of Validation set")

g

print("Lambda min using k-fold-manually")
log(MSPE_val_5$lambda.opt)
print("Minimun MSPE using k-fold-manually")
MSPE_val_5$MSPE.opt
```

Now the k-fold gives much more similar results to both the Lasso and the Ridge regression. The estimate for the optimal lambda is a bit higher but they produce around the same MSPE. 

## 2. A regression model with $p >> n$
Reading in the data.

```{r}
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)
death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
colnames(expr) <- paste("V", 1:nrow(express), sep = "")
```

### 2.1 Lasso estimation using glmnet for regressing 'log.surv' against 'expr'
Glmnet and cv.glmnet are used to obtain the lasso regressino for $log(surv)$ against $express$.

```{r}
set.seed(1234)
lasso_surv <- glmnet(x = expr, y = log.surv,
                     alpha = 1) #default is standardize and intercept TRUE

cv_lasso_surv <- cv.glmnet(x = expr, y = log.surv,
                     alpha = 1)

# Number of non-zero coefficients
cv_lasso_surv
# Plot two graphics
par(mfrow = c(2, 1))
plot(cv_lasso_surv)
plot(lasso_surv, xvar = "lambda")
abline(v=log(cv_lasso_surv$lambda.min), col = 2, lty = 2)
abline(v=log(cv_lasso_surv$lambda.1se), col = 2, lty = 2)
par(mfrow = c(1, 1))
```

There are few non zero coefficiants using Lasso regreassion. As one can see from the MSE plot the number corresponds well with what looks like the point with the lowest MSE. From the coefficient plot one can also see that min lambda results in a vertical line crossing the same number of betapaths. 

### 2.2 Computation of the responding fitted values
The fittet values using our Lasso estimated model is plottet against the observed values.

```{r}
predict_lasso <- predict(cv_lasso_surv,
                  newx = expr,
                  s = cv_lasso_surv$lambda.min)
plot( log.surv, predict_lasso)
abline(a = 0, b = 1, col = 2)
```

As one can see from the plot the real values are on a larger scale than the predicted values. This could mean that this is not the best way to modle this type of data. 

### 2.3 OLS regression model for 'log.surv' against 'expr'
Now we will fitt and OLS model with the responsvariables given by the non-zero coefficiants in the Lasso regression. 

```{r}
coeff_lasso <- rownames(coef(cv_lasso_surv, s = "lambda.min"))[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0]

coeff_lasso


lm_surv <- lm(log.surv ~ expr[, coeff_lasso[-1]])


plot(lm_surv$fitted.values, log.surv)
abline(a = 0, b = 1, col = 2)
```

As one can clearly see this gives a much better prediction for our data. The scales are more similar and the data is quite evenly distributet around the line $x=y$. 

### 2.4 Comparison of Lasso and OLS Regression

```{r, eval=FALSE}
print("Coefficiants Lasso regression")
coef(cv_lasso_surv, s = "lambda.min")[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0]
print("Coefficiants OLS")
lm_surv$coefficients

plot(lm_surv$fitted.values, predict_lasso)
abline(a = 0, b = 1, col = 2)
```

The OLS coefficiants are a lot larger than the Lasso regression coefficiants, this can also be seen in the plot where one can observe that the OLS fittes values are on a much larger scale than the Lasso fitted values. 

