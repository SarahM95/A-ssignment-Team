---
title: "Lasso estimation in multiple linear regression"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
date: "03 de marzo de 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Lasso for the Boston Housing data 
The Boston House-price dataset concerns housing values in 506 suburbs of Boston corresponding to year 1978. They are available here:
   https://archive.ics.uci.edu/ml/datasets/Housing

The Boston House-price corrected dataset (available in boston.Rdata) con- tains the same data (with some corrections) and it also includes the UTM coor- dinates of the geographical centers of each neighborhood.

### 1.1 Lasso estimation using the package 'glmnet'
After loading the right package, the response and explanatory variables from the Boston Housing data are set.
```{r}
#install.packages("glmnet")
library(glmnet)
library(Matrix)

boston <- load("boston.Rdata")

response <- "CMEDV"

explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")

# glmnet cannot handle factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)
```

Fitting the Lasso Regression model.
```{r}

lasso_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = TRUE,
                intercept = TRUE)
lasso_boston$beta

cv_lasso_boston <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = TRUE,
                intercept = TRUE)
cv_lasso_boston
plot(cv_lasso_boston)

library(plotmo) #plot glmnet with coefficient names
plot_glmnet(lasso_boston, s=cv_lasso_boston$lambda.min)
```

The Lasso Estimation does variable selection automatically. This results in a model with around $11$ non-zero explanatory variables and a mean square error around $23$ (changes a bit from run to run). 


### 1.2 Ridge Regression model using glmnet


We fit the dataset using glmnet and ridge regression and plot the results. 
Task description: 


```{r}
ridge_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = TRUE,
                intercept = TRUE)
#ridge_boston$beta 
```

The ridge regression of boston data using cv.glmnet
```{r}
cv_ridge_boston <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = TRUE,
                intercept = TRUE)
#cv_ridge_boston$glmnet.fit$beta

plot_glmnet(ridge_boston, s=cv_ridge_boston$lambda.min)
```

As one can see rigde regression has a lot more non-zero expenatory variables than Lasso. However a lot of them are very close to zero.

The 10-fold-function from previous assignment. 
```{r, warning=FALSE}
# MSE function to be used in MSPE
MSE <- function(Y_val, pred) {
  return(mean((Y_val - pred)^2))
}

MSPE<- function(X_t, Y_t, lambda.v, k_number){
  #lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1 #remove
  n_X_t <- dim(X_t)[1]
  shuffle_index <- sample(1:n_X_t, n_X_t, replace = FALSE)
  max_in_group <- n_X_t %/% k_number
  group_indexes <- split(shuffle_index, ceiling(seq(1:n_X_t) / max_in_group))
  group_indexes[[k_number]] <- c(group_indexes[[k_number]], group_indexes[[k_number + 1]])
  group_indexes[(k_number + 1)] <- NULL
  
  MSPE_groups <- data.frame(matrix(ncol = k_number, nrow = length(lambda.v)))
  
  n_lambdas <- length(lambda.v)
  
  for (k in 1:k_number){
    test <- group_indexes[[k]]
    train <- shuffle_index[!(shuffle_index %in% test)]
    
    #to meet assumption that the predictor variables have zero mean and unit variance 
    #and the response variables have zero mean

    Y<- scale(as.matrix(Y_t[train]), center = TRUE, scale = FALSE)
    X<- scale(as.matrix(X_t[train, ]), center = TRUE, scale = TRUE)
    
    #to be used to scale the validation set.
    center_X <- colMeans(as.matrix(X_t), na.rm = TRUE)
    center_Y <- colMeans(as.matrix(Y_t), na.rm = TRUE)
    scale_X <- sqrt(diag(cov(X_t)))
    
    n <- dim(X)[1]
    p <- dim(X)[2]
    
    XtX <- t(X) %*% X
    beta.path <- matrix(0, nrow = n_lambdas, ncol = p)
    diag.H.lambda <- matrix(0, nrow = n_lambdas, ncol = n)
    
    for (l in 1:n_lambdas) {
      lambda <- lambda.v[l] 
      H <- t(solve(XtX + lambda*diag(1, p)))%*% t(X) 
      beta.path[l, ] <- (H%*%Y)
      #H.lambda <- X %*% H
      #diag.H.lambda[l, ] <- diag(H.lambda)
    }
    
    Y_val <- scale(as.matrix(Y_t[test]), center = center_Y, scale = FALSE)
    X_val <- scale(as.matrix(X_t[test, ]), center = center_X, scale = scale_X)
    
    pred <- X_val %*% t(beta.path)
    MSPE_groups[, k] <- apply(pred, 2, MSE, Y_val = Y_val) #aply MSE to the columns of predict

  }
  MSPE_lambda <- rowMeans(MSPE_groups, na.rm = TRUE)
  
  lambda_opt <- lambda.v[which.min(MSPE_lambda)]
  #print(lambda_opt)
  
  return (list(
    MSPE.all = MSPE_lambda,
    lambda.opt = lambda_opt,
    MSPE.opt = min(MSPE_lambda),
    coeff.lambda = beta.path))
}


lambda.max = 1e9
n_lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1
```


Comparing the ridge regression using R cv_glmnet and our own function.
```{r, warning=FALSE}

plot(cv_ridge_boston)

MSPE_val_5 <- MSPE(X_t = as.matrix(boston.c[, explanatory]), Y_t = boston.c$CMEDV, lambda.v = lambda.v, k = 5)

plot(log(lambda.v), MSPE_val_5$MSPE.all, ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda)", main = "10-fold of Validation set")
abline(v=log(MSPE_val_5$lambda.opt),col=2,lty=2)

print("Lambda min using k-fold-manually")
log(MSPE_val_5$lambda.opt)
```

As shown in the plots the two approchas give a similar $log(lambda)$ value althoug the MSE values are not the same. We assume there is still an error in our k-fold code. 

## 2. A regression model with $p >> n$
Reading in the data.
```{r}
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)
death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
colnames(expr) <- paste("V", 1:nrow(express), sep = "")
```


### 2.1 Lasso estimation using glmnet for regressing 'log.surv' against 'expr'

Glmnet and cv.glmnet are used to obtain the lasso regressino for $log(surv)$ against $express$.

```{r}
set.seed(1234)
lasso_surv <- glmnet(x = expr, y = log.surv,
                     alpha = 1)

cv_lasso_surv <- cv.glmnet(x = expr, y = log.surv,
                     alpha = 1)

# Number of non-zero coefficients
print("Number of non-zero coeff")
length(rownames(coef(cv_lasso_surv, s = "lambda.min"))[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0])

# Plot two graphics
par(mfrow=c(2,1))
plot(cv_lasso_surv)
plot(lasso_surv,xvar="lambda")
abline(v=log(cv_lasso_surv$lambda.min),col=2,lty=2)
abline(v=log(cv_lasso_surv$lambda.1se),col=2,lty=2)
par(mfrow=c(1,1))
```

There are 4 non zero coefficiants using Lasso regreassion. As one can see from the MSE plot this corresponds well with what looks like the point with the lowest MSE. From the coefficient plot one can also see that min lambda results in a vertical line crossing 4 betapaths which corresponds to 4 beta values unequal to zero. 

### 2.2 Computation of the responding fitted values

The fittet values using our Lasso estimated model is plottet against the observed values
```{r}
predict_lasso <- predict(lasso_surv,
                  newx = expr,
                  s = cv_lasso_surv$lambda.min)
plot( log.surv, predict_lasso)
abline(a=0, b=1, col = 2)
```

As one can see from the plot the real values are on a larger scale than the predicted values. This could mean that this is not the best way to modle this type of data. 


### 2.3 OLS regression model for 'log.surv' against 'expr'

Now we will fitt and OLS model with the responsvariables given by the non-zero coefficiants in the Lasso regression. 

```{r}
coeff_lasso <- rownames(coef(cv_lasso_surv, s = "lambda.min"))[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0]

coeff_lasso


lm_surv <- lm(log.surv ~ expr[, coeff_lasso[-1]])


plot(lm_surv$fitted.values, log.surv)
abline(a = 0, b = 1, col = 2)
```

As one can clearly see this gives a much better prediction for our data. The scales are more similar and the data is quite evenly distributet around the line $x=y$. 

### 2.4 Comparison of Lasso and OLS Regression


```{r, eval=FALSE}
print("Coefficiants Lasso regression")
coef(cv_lasso_surv, s = "lambda.min")[coef(cv_lasso_surv, s = "lambda.min")[,1] != 0]
print("Coefficiants OLS")
lm_surv$coefficients

plot(lm_surv$fitted.values, predict_lasso)
abline(a = 0, b = 1, col = 2)
```

The OLS coefficiants are a lot larger than the Lasso regression coefficiants, this can also be seen in the plot where one can observe that the OLS fittes values are on a much larger scale than the Lasso fitted values. 

