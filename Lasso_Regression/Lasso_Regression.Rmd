---
title: "Lasso estimation in multiple linear regression"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
date: "03 de marzo de 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Lasso for the Boston Housing data 
The Boston House-price dataset concerns housing values in 506 suburbs of Boston corresponding to year 1978. They are available here:
   https://archive.ics.uci.edu/ml/datasets/Housing

The Boston House-price corrected dataset (available in boston.Rdata) con- tains the same data (with some corrections) and it also includes the UTM coor- dinates of the geographical centers of each neighborhood.

### 1.1 Lasso estimation using the package 'glmnet'
After loading the right package, the response and explanatory variables from the Boston Housing data are set.
```{r}
#install.packages("glmnet")
library(glmnet)
library(Matrix)

boston <- load("boston.Rdata")

response <- "CMEDV"

explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")

# glmnet cannot handle factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)
```

Task description: 

For the Boston House-price corrected dataset use Lasso estimation (in glmnet) to fit the regression model where the response is CMEDV (the co- rrected version of MEDV) and the explanatory variables are the remaining 13 variables in the previous list. Try to provide an interpretation to the estimated model.


Fitting the Lasso Regression model.
```{r}
lasso_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 1,     # specifying alpha = 1: Lasso Regression
                standardize = FALSE,
                intercept = FALSE)
lasso_boston
plot(lasso_boston)


```


### 1.2 Ridge Regression model using glmnet
Task description: 

Use glmnet to fit the previous model using ridge regression. Compare the 10-fold cross validation results from function cv.glmnet with those you obtained in the previous practice with your own functions.

```{r}
ridge_boston <- glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = FALSE,
                intercept = FALSE)
ridge_boston 
plot(ridge_boston)



```

The ridge regression of boston data using cv.glmnet
```{r}
ridge_boston_cv <- cv.glmnet(x = as.matrix(boston.c[, explanatory]), 
                y = boston.c$CMEDV,
                alpha = 0,    # specifying alpha = 0: Ridge Regression
                standardize = FALSE,
                intercept = FALSE)

```

The function that we have used in the other practice
```{r}
PMSE_k_fold <- function(X_t, Y_t, lambda.v, k=10){

  n_X_t <- dim(X_t)[1]
  p_X_t <- dim(X_t)[2] #length of columns
  n_subset <- as.integer((n_X_t)/k)+ 1
  group <- rep(seq(1,k), times = n_subset)
  group <- group[1:n_X_t]
group_random <- sample(group)
X_t_group <- cbind(X_t, group_random)
Y_t_group <- cbind(Y_t, group_random)

#now we can start:

PMSE <- list()

for (la in 1:n_lambdas){

  group_random <- sample(group)

  X_t_group <- cbind(X_t, group_random)

  Y_t_group <- cbind(Y_t, group_random)

  #now we can start:

  PMSE <- list()

  for (la in 1:n_lambdas){

    lambda <- lambda.v[la]
    
    y_hat <- list()
    beta <- list() 
    h <- list()
    y <- list()

    for (l in 1:k){

      new_X_t_val <- subset(X_t_group, group_random==l)[ ,1:p_X_t]
      new_X_t_test <- subset(X_t_group, group_random!=l)[ ,1:p_X_t]

      new_Y_t_val <- subset.matrix(Y_t_group, group_random==l)[,1]
      new_Y_t_test <- subset.matrix(Y_t_group, group_random!=l)[,1]

      p_new <- dim(new_X_t_test)[2]
      p_new_v <- dim(new_X_t_val)[2]

      beta[[l]] <- solve(t(new_X_t_test)%*%new_X_t_test + lambda*diag(1,p_new))%*% t(new_X_t_test)%*%(new_Y_t_test)

      H_val <- new_X_t_val%*%solve(t(new_X_t_val)%*%new_X_t_val + (lambda+1e-13)*diag(1,p_new_v))%*% t(new_X_t_val) 

      # singular matrix for lambda = 0 -> trick: add a very small number 

      y_hat[[l]] <-  (new_X_t_val)%*%beta[[l]]
      h[[l]] <- diag(H_val)
      y[[l]] <- new_Y_t_val 

      
  }

  y_hat <- c(do.call(rbind, y_hat))
  beta <- c(do.call(cbind, beta))
  h <- c(do.call(rbind, h))
  y <- c(do.call(rbind, y))

  PMSE[[la]] <- 1/n_X_t * sum(((y-y_hat)/(1-h))^2)

  }

}

return(PMSE)

}
lambda.max = 2e4
n_lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1
```

```{r}
ridge_boston_cv
plot(ridge_boston_cv)


PMSE_val_10 <- PMSE_k_fold(X_t = as.matrix(boston.c[, explanatory]), Y_t = boston.c$CMEDV, lambda.v = lambda.v, k = 10)
plot(log(lambda.v[-1]+1), PMSE_val_10[-1], ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda+1)", main = "10-fold of Validation set")
#I am not sure if we have to keep or skip the +1 of this plot in the log(lambda)

```


## 2. A regression model with $p >> n$
Reading in the data.
```{r}
express <- read.csv("journal.pbio.0020108.sd012.CSV",header=FALSE)
surv <- read.csv("journal.pbio.0020108.sd013.CSV",header=FALSE)
death <- (surv[,2]==1)
log.surv <- log(surv[death,1]+.05)
expr <- as.matrix(t(express[,death]))
```


### 2.1 Lasso estimation using glmnet for regressing 'log.surv' against 'expr'
Task decsription: 

Use glmnet and glmnet to obtain the Lasso estimation for regressing log.surv against expr. How many coefficient different from zero are in the Lasso estimator? Illustrate the result with two graphics.

```{r}
lasso_surv <- glmnet(x = expr, y = log.surv,
                     alpha = 1)
lasso_surv
plot(lasso_surv)

lasso_surv_cv <- cv.glmnet(x = expr, y = log.surv,
                     alpha = 1)
lasso_surv_cv
plot(lasso_surv_cv)

```


### 2.2 Computation of the responding fitted values
Task description: 

Compute the fitted values with the Lasso estimated model (you can use predict). Plot the observed values for the response variable against the Lasso fitted values.

```{r}
?predict

```



### 2.3 OLS regression model for 'log.surv' against 'expr'
Task description: 

Consider the set S0 of non-zero estimated Lasso coefficients. Use OLS to fit a regression model with response log.surv and explanatory variables the columns of expr with indexes in S0. Plot the observed values for the response variable against the OLS fitted values.

```{r}
lm_surv <- lm(log.surv ~ expr)
lm_surv

plot(log.surv, lm_surv$fitted.values)
```


### 2.4 Comparison of Lasso and OLS Regression
Task description: 

Compare the OLS and Lasso fitted values. Do a plot for that.

```{r, eval=FALSE}
plot(lm_surv$fitted.values, lasso_surv) # fitted values of lasso??
```



