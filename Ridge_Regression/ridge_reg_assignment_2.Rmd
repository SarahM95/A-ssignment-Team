---
title: "Ridge Regression"
date: "25/02/20"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Choosing the penalization parameter

Testdate (to be removed later!)
```{r}
prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)
plot(prostate)
train.sample <- which(prostate$train==TRUE) ##separate trainingsdata from testdata
val.sample <- which(prostate$train==FALSE)


Y_t <- scale( prostate$lpsa[train.sample], center=TRUE, scale=FALSE) ## center but not scale for response
X_t <- scale( as.matrix(prostate[train.sample,1:8]), center=TRUE, scale=TRUE) ##scale and center for

Y_val <- scale( prostate$lpsa[val.sample], center=TRUE, scale=FALSE) ## center but not scale for response
X_val <- scale( as.matrix(prostate[val.sample,1:8]), center=TRUE, scale=TRUE)

#predictors
p <- dim(X_t)[2]

XtX <- t(X_t)%*%X_t 
d2 <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values #eigenvalues of xtx

(cond.number <- sqrt(max(d2)/min(d2)))

lambda.max = 2e4
n_lambdas <- 25 ## look at 25 different values
lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1 #lambda vector

n_val <- length(Y_val)
```

```{r}
PMSE_vs <- function(X_t, Y_t, X_val, Y_val, lambda){
  p <- dim(X_t)[2]
  n_lambdas <- length(lambda)

  XtX <- t(X_t)%*%X_t 

  
  PMSE_vec <- vector("numeric", length = n_lambdas)
  for(l in 1:n_lambdas){
    lambda <- lambda.v[l]
    beta_hat <- solve(XtX + lambda*diag(1,p)) %*% t(X_t) %*% Y_t
    #y_hat = X %*% beta_hat
    m_hat_vec <- vector("numeric", length = n_val)
    
    for (n in 1:n_val){
      m_hat_vec[n] <- (Y_val[n]-(X_val[n,]%*%beta_hat))^2
    }
    PMSE_vec[l]<- sum(m_hat_vec)/n_val
    
  }
  return (PMSE_vec)
}
```


```{r}
PMSE_vec_test <- PMSE_vs(X_t, Y_t, X_val, Y_val, lambda.v)

lambda.CV <- lambda.v[which.min(PMSE_vec_test)]
plot(log(1+lambda.v), PMSE_vec_test)
abline(v=log(1+lambda.CV),col=2,lty=2)

```


##Task 2

```{r}
X_t_shuffel <- X_t[sample(nrow(X_t)),]


k = 10

is.matrix(X_t_shuffel)

#Assume we can delete the last ? values
length(X_t_shuffel)
n_subset = as.integer(length(X_t_shuffel)/k)
n_subset

k_subsets_matrix <- matrix(0, nrow=k, ncol = n_subset)

c = 1
for (i in 1:k){
  for (j in 1:n_subset){
    k_subsets_k <- X_t_shuffel[(n_subset*(i-1)+1):(n_subset*i)]
    c<- c + 1
  }
}

k_subsets_matrix

    

```

```{r, warning=FALSE}
#Different aproach

k=10
n_X_t <- dim(X_t)[1]
p_X_t <- dim(X_t)[2] #length of columns
n_subset <- as.integer((n_X_t)/k)+ 1
group <- rep(seq(1,k), times = n_subset)
group <- group[1:n_X_t]

group_random <- sample(group)

X_t_group <- cbind(X_t, group_random)
Y_t_group <- cbind(Y_t, group_random)


#now we can start:
PMSE <- list()

for (la in 1:n_lambdas){
  
  lambda <- lambda.v[la]
  
  y_hat <- list()
  beta <- list()
  h <- list()
  y <- list()
  
  for (l in 1:k){
    new_X_t_val <- subset(X_t_group, group_random==l)[ ,1:p_X_t]
    new_X_t_test <- subset(X_t_group, group_random!=l)[ ,1:p_X_t]
    
    new_Y_t_val <- subset.matrix(Y_t_group, group_random==l)[,1]
    new_Y_t_test <- subset.matrix(Y_t_group, group_random!=l)[,1]
    
    p_new <- dim(new_X_t_test)[2]
    p_new_v <- dim(new_X_t_val)[2]
    #H.i <- new_X_t_test%*%solve(t(new_X_t_test)%*%new_X_t_test + 1*diag(1,p_new))%*% t(new_X_t_test)
    beta[[l]] <- solve(t(new_X_t_test)%*%new_X_t_test + lambda*diag(1,p_new))%*% t(new_X_t_test)%*%(new_Y_t_test)
    #y_hat.i <- H.i %*%(new_Y_t_test)
    H_val <- new_X_t_val%*%solve(t(new_X_t_val)%*%new_X_t_val + (lambda+0.000000001)*diag(1,p_new_v))%*% t(new_X_t_val) 
    # singular matrix for lambda = 0 -> trick: add a very small number 
    y_hat[[l]] <-  (new_X_t_val)%*%beta[[l]]
    h[[l]] <- diag(H_val)
    y[[l]] <- new_Y_t_val 
    
    #fill in to vectors outside

  }
  y_hat <- c(do.call(rbind, y_hat))
  beta <- c(do.call(cbind, beta))
  h <- c(do.call(rbind, h))
  y <- c(do.call(rbind, y))
  
  PMSE[[la]] <- 1/n_X_t * sum(((y-y_hat)/(1-h))^2)
  #PMSE_vec[l]<- #sum(m_hat_vec)/n_val #not this but something
}    
PMSE

plot((log(lambda.v[-1]+1)), PMSE[-1])
```


## Ridge Regression for the Boston Housing data

Loading the (corrected) Boston Housing data
```{r}
library(MASS)
data(Boston)
help(Boston)

boston <- load("boston.Rdata")
```


There is a package for Regularization. We are probably not supposed to use this...
```{r}
#install.packages("glmnet")
library(glmnet)

response <- "MEDV"
explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")

# cv.glmnet cannot handel factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)

(ridge <- glmnet(y = boston.c$MEDV, x = as.matrix(boston.c[, explanatory]), alpha = 0))
# alpha = 0: Ridge Regression
# alpha = 1: Lasso Regression
plot(ridge)

(cv.ridge <- cv.glmnet(y = boston.c$MEDV, x = as.matrix(boston.c[, explanatory]), alpha = 0))
plot(cv.ridge)
```


