---
title: "Ridge Regression"
date: "25/02/20"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Choosing the penalization parameter

Testdate (to be removed later!)
```{r, eval = FALSE}
prostate <- read.table("/Users/aurorahofman/Documents/Utveksling/Stat_lÃ¦r/week_2/prostate_data.txt", header=TRUE, row.names = 1)
plot(prostate)
train.sample <- which(prostate$train==TRUE) ##separate trainingsdata from testdata
val.sample <- which(prostate$train==FALSE)


Y_t <- scale( prostate$lpsa[train.sample], center=TRUE, scale=FALSE) ## center but not scale for response
X_t <- scale( as.matrix(prostate[train.sample,1:8]), center=TRUE, scale=TRUE) ##scale and center for

Y_val <- scale( prostate$lpsa[val.sample], center=TRUE, scale=FALSE) ## center but not scale for response
X_val <- scale( as.matrix(prostate[val.sample,1:8]), center=TRUE, scale=TRUE)

#predictors
p <- dim(X_t)[2]

XtX <- t(X_t)%*%X_t 
d2 <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values #eigenvalues of xtx

(cond.number <- sqrt(max(d2)/min(d2)))

lambda.max = 2e4
n_lambdas <- 25 ## look at 25 different values
lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1 #lambda vector

n_val <- length(Y_val)
```

```{r, eval = FALSE}



```


```{r, eval = FALSE}
PMSE_vs <- function(X_t, Y_t, X_val, Y_val, lambda){
  p <- dim(X_t)[2]
  n_lambdas <- length(lambda)

  XtX <- t(X_t)%*%X_t 

  
  PMSE_vec <- vector("numeric", length = n_lambdas)
  for(l in 1:n_lambdas){
    lambda <- lambda.v[l]
    beta_hat <- solve(XtX + lambda*diag(1,p)) %*% t(X_t) %*% Y_t
    #y_hat = X %*% beta_hat
    m_hat_vec <- vector("numeric", length = n_val)
    
    for (n in 1:n_val){
      m_hat_vec[n] <- (Y_val[n]-(X_val[n,]%*%beta_hat))^2
    }
    PMSE_vec[l]<- sum(m_hat_vec)/n_val
    
  }
  return (PMSE_vec)
}
```


```{r}
PMSE_vec_test <- PMSE_vs(X_t, Y_t, X_val, Y_val, lambda.v)

lambda.CV <- lambda.v[which.min(PMSE_vec_test)]
plot(log(1+lambda.v), PMSE_vec_test)
abline(v=log(1+lambda.CV),col=2,lty=2)

```


##Task 2

```{r}
X_t_shuffel <- X_t[sample(nrow(X_t)),]


k = 10

is.matrix(X_t_shuffel)

#Assume we can delete the last ? values
length(X_t_shuffel)
n_subset = as.integer(length(X_t_shuffel)/k)
n_subset

k_subsets_matrix <- matrix(0, nrow=k, ncol = n_subset)

c = 1
for (i in 1:k){
  for (j in 1:n_subset){
    k_subsets_k <- X_t_shuffel[(n_subset*(i-1)+1):(n_subset*i)]
    c<- c + 1
  }
}

k_subsets_matrix

    

```
```{r}
#Different aproach

k=10
n_X_t <- dim(X_t)[1]
n_subset <- as.integer((n_X_t)/k)+ 1
group <- rep(seq(1,k), times = n_subset)
group <- group[1:n_X_t]

group_random <- sample(group)

X_t_group <- cbind(X_t, group_random)
Y_t_group <- cbind(Y_t, group_random)


#now we can start:

nex_X_t_val <- subset.matrix(X_t_group, group_random==1)
new_X_t_test <- subset.matrix(X_t_group, group_random!=1)

nex_Y_t_val <- subset.matrix(Y_t_group, group_random==1)
new_Y_t_test <- subset.matrix(Y_t_group, group_random!=1)

p_new <- dim(new_X_t_test)[2]
beta.i <- solve(t(new_X_t_test)%*%new_X_t_test + 1*diag(1,p_new)) %*% t(new_X_t_test) %*%new_Y_t_test
beta.i
for (k in 1:k){
  nex_X_t_val <- subset.matrix(X_t_group, group_random==k)
  new_X_t_test <- subset.matrix(X_t_group, group_random!=k)
  
  nex_Y_t_val <- subset.matrix(Y_t_group, group_random==k)
  new_Y_t_test <- subset.matrix(Y_t_group, group_random!=k)
  
  lambda <- lambda.v[1]
  PMSE <- 0
  for (i in 1:k){ #considder al possible data
    #m.Y.i <- mean(Y[-i]) ##use this for unscaled data
    m.Y.i <- 0 ##if the data is scaled use this
    X.i <- X[-i,]; Y.i <- Y[-i]-m.Y.i #take out the ith observetion
    Xi <- X[i,]; Yi <- Y[i]
    beta.i <- solve(t(X.i)%*%X.i + lambda_v[1]*diag(1,p)) %*% t(X.i) %*% Y.i #compute beta 
    hat.Yi <- Xi %*% beta.i + m.Y.i #compute hat
    PMSE.CV[l] <- PMSE.CV[l] + (hat.Yi-Yi)^2
  }
  PMSE.CV[l] <- PMSE.CV[l]/n
}
  XtX_new <- t(new_X_t_test)%*%new_X_t_test 

  
  PMSE_vec <- vector("numeric", length = n_lambdas)
  for(l in 1:n_lambdas){
    lambda <- lambda.v[l]
    beta_hat <- solve(XtX + lambda*diag(1,p)) %*% t(X_t) %*% Y_t
    #y_hat = X %*% beta_hat
    m_hat_vec <- vector("numeric", length = n_val)
    
    for (n in 1:n_val){
      m_hat_vec[n] <- (Y_val[n]-(X_val[n,]%*%beta_hat))^2
    }
    PMSE_vec[l]<- sum(m_hat_vec)/n_val
    
  }






```


## Ridge Regression for the Boston Housing data

Loading the (corrected) Boston Housing data
```{r}
library(MASS)
data(Boston)
help(Boston)

boston <- load("boston.Rdata")
```


There is a package for Regularization. We are probably not supposed to use this...
```{r}
#install.packages("glmnet")
library(glmnet)

response <- "MEDV"
explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")

# cv.glmnet cannot handel factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)

(ridge <- glmnet(y = boston.c$MEDV, x = as.matrix(boston.c[, explanatory]), alpha = 0))
# alpha = 0: Ridge Regression
# alpha = 1: Lasso Regression
plot(ridge)

(cv.ridge <- cv.glmnet(y = boston.c$MEDV, x = as.matrix(boston.c[, explanatory]), alpha = 0))
plot(cv.ridge)
```


