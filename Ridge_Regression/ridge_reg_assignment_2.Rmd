---
title: "Ridge Regression"
date: "25/02/20"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For the Assignment about Ridge Regression, we compute a function to choose a penalization parameter. The theory about Ridge Regression is used to write basic functions. In order to test these functions the Boston Housing data is used with it.
Alternatively, we proposed to use a package that deals with Ridge Regression in the background.

## Choosing the penalization parameter

### Function for choosing the penalization parameter
```{r}
prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)
train.sample <- which(prostate$train==TRUE) ##separate trainingsdata from testdata
val.sample <- which(prostate$train==FALSE)


Y_t <- scale( prostate$lpsa[train.sample], center=TRUE, scale=FALSE) ## center but not scale for response
X_t <- scale( as.matrix(prostate[train.sample,1:8]), center=TRUE, scale=TRUE) ##scale and center for

Y_val <- scale( prostate$lpsa[val.sample], center=TRUE, scale=FALSE) ## center but not scale for response
X_val <- scale( as.matrix(prostate[val.sample,1:8]), center=TRUE, scale=TRUE)

#predictors
p <- dim(X_t)[2]

XtX <- t(X_t)%*%X_t 
d2 <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values #eigenvalues of xtx

(cond.number <- sqrt(max(d2)/min(d2)))

lambda.max = 2e4
n_lambdas <- 25 ## look at 25 different values
lambda.v <- exp(seq(0,log(lambda.max+1),length=n_lambdas))-1 #lambda vector

lambda.v

n_val <- length(Y_val)
```

```{r}
PMSE_vs <- function(X_t, Y_t, X_val, Y_val, lambda){
  p <- dim(X_t)[2]
  n_lambdas <- length(lambda)

  XtX <- t(X_t)%*%X_t 

  
  PMSE_vec <- vector("numeric", length = n_lambdas)
  for(l in 1:n_lambdas){
    lambda <- lambda.v[l]
    beta_hat <- solve(XtX + lambda*diag(1,p)) %*% t(X_t) %*% Y_t
    #y_hat = X %*% beta_hat
    m_hat_vec <- vector("numeric", length = n_val)
    
    for (n in 1:n_val){
      m_hat_vec[n] <- (Y_val[n]-(X_val[n,]%*%beta_hat))^2
    }
    PMSE_vec[l]<- sum(m_hat_vec)/n_val
    
  }
  return (PMSE_vec)
}
```


```{r}
PMSE_vec_test <- PMSE_vs(X_t, Y_t, X_val, Y_val, lambda.v)

lambda.CV <- lambda.v[which.min(PMSE_vec_test)]
plot(log(1+lambda.v), PMSE_vec_test)
abline(v=log(1+lambda.CV),col=2,lty=2)

```


### Function for Implementing the Ridge Regression penalization parameter
```{r, warning = FALSE}
PMSE_k_fold <- function(X_t, Y_t, lambda.v, k=10){
  n_X_t <- dim(X_t)[1]
  p_X_t <- dim(X_t)[2] #length of columns
  n_subset <- as.integer((n_X_t)/k)+ 1
  group <- rep(seq(1,k), times = n_subset)
  group <- group[1:n_X_t]

<<<<<<< Updated upstream
=======
```{r}
X_t_shuffel <- X_t[sample(nrow(X_t)),]


k = 10

is.matrix(X_t_shuffel)

#Assume we can delete the last ? values
length(X_t_shuffel)
n_subset = as.integer(length(X_t_shuffel)/k)
n_subset

k_subsets_matrix <- matrix(0, nrow=k, ncol = n_subset)

c = 1
for (i in 1:k){
  for (j in 1:n_subset){
    k_subsets_k <- X_t_shuffel[(n_subset*(i-1)+1):(n_subset*i)]
    c<- c + 1
  }
}

k_subsets_matrix

    

```
```{r}
#Different aproach

k=10
n_X_t <- dim(X_t)[1]
p_X_t <- dim(X_t)[2]
n_subset <- as.integer((n_X_t)/k)+ 1
group <- rep(seq(1,k), times = n_subset)
group <- group[1:n_X_t]
>>>>>>> Stashed changes

group_random <- sample(group)

X_t_group <- cbind(X_t, group_random)
Y_t_group <- cbind(Y_t, group_random)


#now we can start:
PMSE <- list()

for (la in 1:n_lambdas){
  
  group_random <- sample(group)
  
<<<<<<< Updated upstream
  X_t_group <- cbind(X_t, group_random)
  Y_t_group <- cbind(Y_t, group_random)


  #now we can start:
  PMSE <- list()
  
  for (la in 1:n_lambdas){
    
    lambda <- lambda.v[la]
    
    y_hat <- list()
    beta <- list() 
    h <- list()
    y <- list()
    
    for (l in 1:k){
      new_X_t_val <- subset(X_t_group, group_random==l)[ ,1:p_X_t]
      new_X_t_test <- subset(X_t_group, group_random!=l)[ ,1:p_X_t]
      
      new_Y_t_val <- subset.matrix(Y_t_group, group_random==l)[,1]
      new_Y_t_test <- subset.matrix(Y_t_group, group_random!=l)[,1]
      
      p_new <- dim(new_X_t_test)[2]
      p_new_v <- dim(new_X_t_val)[2]
      
      beta[[l]] <- solve(t(new_X_t_test)%*%new_X_t_test + lambda*diag(1,p_new))%*% t(new_X_t_test)%*%(new_Y_t_test)
      
      H_val <- new_X_t_val%*%solve(t(new_X_t_val)%*%new_X_t_val + (lambda+1e-13)*diag(1,p_new_v))%*% t(new_X_t_val) 
      # singular matrix for lambda = 0 -> trick: add a very small number 
      y_hat[[l]] <-  (new_X_t_val)%*%beta[[l]]
      h[[l]] <- diag(H_val)
      y[[l]] <- new_Y_t_val 
      
 
=======
  H_ii_vec <- vector("numeric", length = n_X_t)
  Y_hat_vec <- vector("numeric", length = n_X_t)
  Y_vec <- vector("numeric", length = n_X_t)
  
  y_hat <- list()
  
  
  for (l in 1:k){
    nex_X_t_val <- subset.matrix(X_t_group, group_random==l) [ ,1:p_X_t]
    new_X_t_test <- subset.matrix(X_t_group, group_random!=l)[ ,1:p_X_t]
    
    nex_Y_t_val <- subset.matrix(Y_t_group, group_random==l)[,1]
    new_Y_t_test <- subset.matrix(Y_t_group, group_random!=l)[,1]
    
    p_new <- dim(new_X_t_test)[2]
    #H.i <- new_X_t_test%*%solve(t(new_X_t_test)%*%new_X_t_test + 1*diag(1,p_new))%*% t(new_X_t_test)
    Beta.i <- solve(t(new_X_t_test)%*%new_X_t_test + (lambda)*diag(1,p_new) )%*% t(new_X_t_test)%*%(new_Y_t_test)
    #y_hat.i <- H.i %*%(new_Y_t_test)
    H_i_val <- new_X_t_val%*%solve(t(new_X_t_val)%*%new_X_t_val + lambda*diag(1,p_new_v))%*% t(new_X_t_val)
    y_hat[[i]] <-  (new_X_t_val)%*%Beta.i
    h_ii <- diag(H_i_val)
    
    
    #fill in to vectors outside
      
>>>>>>> Stashed changes
  }
  list <- c(do.call(cbind, y_hat))
  
  PMSE[[la]] <- 1/n_X_t * sum(((y-y_hat)/(1-h))^2)
  }
}
return(PMSE)
}

PMSE <- PMSE_k_fold(X_t = X_t, Y_t = Y_t, lambda.v = lambda.v, k = 10)
plot(log(lambda.v[-1]+1), PMSE[-1], ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda+1)", main = "10-fold of Training set")
```


### Comparison between 5-fold and 10-fold 
```{r}
PMSE_val_5 <- PMSE_k_fold(X_t = X_val, Y_t = Y_val, lambda.v = lambda.v, k = 5)
plot(log(lambda.v[-1]+1), PMSE_val_5[-1], ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda+1)", main = "5-fold of Validation set")

PMSE_val_10 <- PMSE_k_fold(X_t = X_val, Y_t = Y_val, lambda.v = lambda.v, k = 10)
plot(log(lambda.v[-1]+1), PMSE_val_10[-1], ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda+1)", main = "10-fold of Validation set")

plot(log(1+lambda.v), PMSE_vec_test)
abline(v=log(1+lambda.CV),col=2,lty=2)
```



## Ridge Regression for the Boston Housing data

Loading the (corrected) Boston Housing data
```{r}
library(MASS)
data(Boston)
help(Boston)

<<<<<<< Updated upstream
boston <- load("boston.Rdata")

response <- "MEDV"
explanatory <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT")
=======
Boston
>>>>>>> Stashed changes
```

### Using obtained functions for Boston Housing data
```{r, warning = FALSE}
PMSE_10 <- PMSE_k_fold(X_t = as.matrix(boston.c[, explanatory]), Y_t = boston.c$MEDV, lambda.v = lambda.v, k = 10)
plot(log(lambda.v[-1]+1), PMSE_10[-1], ylab = "Predicted Mean Squarred Error", xlab = "log(Lambda+1)", main = "10-fold for Boston Housing Data")
```


### Alternative solution for Ridge Regression for the Boston Housing data
There exists a package called 'glmnet' that deals with elastic nets. Specifying $alpha = 0$ Ridge Regression is applied on the data. 
```{r}
#install.packages("glmnet")
library(glmnet)

# cv.glmnet cannot handel factors -> "CHAS" is a factor
boston.c$CHAS <- as.numeric(boston.c$CHAS)

(ridge <- glmnet(y = boston.c$MEDV, x = as.matrix(boston.c[, explanatory]), alpha = 0))
# alpha = 0: Ridge Regression
# alpha = 1: Lasso Regression
plot(ridge)

(cv.ridge <- cv.glmnet(y = boston.c$MEDV, x = as.matrix(boston.c[, explanatory]), alpha = 0))
plot(cv.ridge)
```



```{r}
#manually

is.data.frame(Boston)

Y <- scale( Boston$medv, center=TRUE, scale=FALSE) ## center but not scale for response
X <- scale( as.matrix(Boston[1:13]), center=TRUE, scale=TRUE) ##scale and center for
p <- dim(X)[2]

#find lambda using kfold validation.

#k_fold(X, )

lambda.opt <- lambda.v[which.min(PMSE_vec_test)] # someting like this

Beta_optimal <- solve(t(X)%*%X + (lambda.opt)*diag(1,p ))%*% t(X)%*%(Y)
                      
cbind(explanatory, Beta_optimal)


```
Show in New WindowClear OutputExpand/Collapse Output

[1] 4.435608
R Console

Modify Chunk OptionsRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current Chunk
Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output
Error: unexpected '}' in:
"    
}"
