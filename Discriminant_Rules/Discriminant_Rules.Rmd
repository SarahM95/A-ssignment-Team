---
title: "Comparing discriminant rules. ROC curve and other methods"
author: "Gregoire Gasparini, Aurora Hofman, Sarah Musiol, Beatriu Tort"
date: "07 de marzo de 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## From the description file:
The “spam” concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography... Our collection of spam e- mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word ’george’ and the area code ’650’ are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.


## Attribute Information:
-The last column of ’spambase.data’ denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.
-Most of the attributes indicate whether a particular word or character was frequently occurring in the e-mail.
-The run-length attributes (55-57) measure the length of sequences of consecutive capital letters.


## 1. Use the script spam.R to read the data from the SPAM e-mail database.

```{r, eval=FALSE}
source("Discriminant_Rules/spam_email_database/spam.R")
# why is this not working????
```

```{r}
spam <- read.table("spam_email_database/spambase.data", sep = ",")

spam.names <-
  c(
    read.table(
      "spam_email_database/spambase.names",
      sep = ":",
      skip = 33,
      nrows = 53,
      as.is = TRUE
    )[, 1],
    "char_freq_#",
    read.table(
      "spam_email_database/spambase.names",
      sep = ":",
      skip = 87,
      nrows = 3,
      as.is = TRUE
    )[, 1],
    "spam.01"
  )

names(spam) <- spam.names 
```


## 2. Dividing data in training set and test set
Task description: 

Divide the data into two parts: 2/3 for the training sample, 1/3 for the test sample. You should do it in a way that SPAM e-mail are 2/3 in the training sample and 1/3 in the test sample, and that the same happens for NO SPAM e-mails.

```{r}
ind_train_spam <- as.numeric(sample(rownames(spam[spam$spam.01 == 1,]), 2 / 3 * nrow(spam[spam$spam.01 == 1,])))
ind_train_nospam <- as.numeric(sample(rownames(spam[spam$spam.01 == 0,]), 2 / 3 * nrow(spam[spam$spam.01 == 0,])))

spam_train <- spam[c(ind_train_spam, ind_train_nospam),]
spam_test <- spam[-c(ind_train_spam, ind_train_nospam),]
```


## 3. Classification using the Training sample
Task description: 

Consider the following three classification rules:

-Logistic regression fitted by maximum likelihood (IRWLS, glm). 
-Logistic regression fitted by Lasso (glment).
-k-nn binary regression (you can use your own implementation or functions knn and knn.cv from the R package class).

Use the training sample to fix the tunning parameters (when needed) and to estimate the model parameters (when needed).


### Logistic Regression by Maximum Likelihood
```{r, eval=FALSE}
response <- "spam.01"
explanatory <-
  colnames(spam_train)[colnames(spam_train) != response]
# Logistic regression by maximum likelihood
glm_spam <-
  glm(spam_train$spam.01 ~ ., family = binomial(link = "logit"), data = spam_train)
```

### Logistic Regression by Lasso
```{r,eval=FALSE}
library(glmnet)
library(Matrix)

glmnet_spam <-
  glmnet(as.matrix(spam_train[, explanatory]), spam_train$spam.01, family = "binomial")

cv_glmnet_spam <-
  cv.glmnet(as.matrix(spam_train[, explanatory]), spam_train$spam.01, family = "binomial")
```


### KNN binary regression
```{r,eval=FALSE}
# KNN binary regression 
library(class)

cl <- factor(c(rep("0", sum(spam_train$spam.01 == 0)), rep("1", sum(spam_train$spam.01 == 1))))
knn_spam <- knn(spam_train, spam_test, cl, prob = TRUE)
cv_knn_spam <- knn.cv(spam_train, cl, k=1)
```


## 4. Use the test sample to compute and plot the ROC curve for each rule.
ROC curve: y-axis Sensitivity
            x-axis 1-Specificity
            
Sensitivity: Probability of classifying correctly a positive
case: TRUE POSITIVE.
Specificity: Probability of classifying correctly a negative case.
### ROC curve for GLM
```{r}
library(ROCR)
# Complicated version:
glm_pred <- predict(glm_spam, newdata = spam_test)
glm_prediction <- prediction(glm_pred, spam_test$spam.01)
glm_performance <- performance(glm_prediction, "sens", "spec")
```

### ROC curve for glmnet
```{r}
glmnet_pred <- predict(cv_glmnet_spam, newx = as.matrix(spam_test[, explanatory]), type = "response")
glmnet_prediction <- prediction(glmnet_pred, spam_test$spam.01)
glmnet_performance <- performance(glmnet_prediction, "sens", "spec")
```

### ROC curve for KNN
```{r}
# copied from https://stackoverflow.com/questions/11741599/how-to-plot-a-roc-curve-for-a-knn-model
#remotes::install_github("Dasonk/knnflex")
library(knnflex)

#knn_dist <- knn.dist(spam)
#knn_pred <- knn.predict(spam_train, spam_test, y = spam.01, knn_dist, k=3)
prob <- attr(knn_spam, "prob")
knn_pred <- ifelse(knn_spam == "-1", 1-prob, prob) - 1

cl_test <- factor(c(rep("0", sum(spam_test$spam.01 == 0)), rep("1", sum(spam_test$spam.01 == 1))))
knn_prediction <- prediction(knn_pred, cl_test)
knn_performance <- performance(knn_prediction, "sens", "spec")
```

### Plot ROC curves for each rule
```{r}
plot(glm_performance, col = "red")
plot(glmnet_performance, add = TRUE, col = "green")
plot(knn_performance, add = TRUE, col = "black")
legend("bottomleft", legend("GLM", "GLMNET", "KNN"), col = c("red", "green", "black"))
```




## 5. Compute also the misclassification rate for each rule when using the cut point c = 1/2.

```{r}
c = 1/2
library(shipunov)

# GLM 
class_glm <- as.numeric(ifelse(glm_pred > 0.5,
                           "1", "0"))

MCR_glm <- Misclass(class_glm, spam_test$spam.01)

# GLMNET
class_glmnet <- as.numeric(ifelse(glmnet_pred > 0.5,
                           "1", "0"))

MCR_glmnet <- Misclass(class_glmnet, spam_test$spam.01)

# KNN 
class_knn <- as.numeric(ifelse(knn_pred > 0.5,
                           "1", "0"))

MCR_knn <- Misclass(class_knn, spam_test$spam.01)
```

### Plot the Misclassification Error Rates for each rule
```{r}

```



## 6. Compute $l_{val}$ for each rule.

```{r}
likelihood <- function(test, y, MCR) {
  prob <- (MCR[2,1] + MCR[2,2])/nrow(test)
  l_val <- 1/nrow(test) * sum(y * log(prob) + (1-y) * log(1-prob))
  return(l_val)
}

# GLM
l_val_glm <- likelihood(spam_test, spam_test$spam.01, MCR_glm)

# GLMNET
l_val_glmnet <- likelihood(spam_test, spam_test$spam.01, MCR_glmnet)

# KNN
l_val_knn <- likelihood(spam_test, spam_test$spam.01, MCR_knn)
```

