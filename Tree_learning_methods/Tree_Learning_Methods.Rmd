---
title: "Tree learnign methods"
output: html_document
params:
  show_code: TRUE #possibility to not show the code just the results
  seed: 1234
  author: 'Gregoire Gasparini, Aurora Hofman, Beatriu Tort'
  partition: 0.5
  myDescription: 
  myfile: "soldat.csv"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(caret)
library(tree)
```

```{r}
data <- read.csv(params$myfile)
```

```{r}
summary(data)
```

In the summary we can see that the only variable that has missing values is $x71$ with a percentage of $13.97$%. To preform analysis one can not have missing values hence we decided to substitute the NA values with the column mean in stead of for example deleting the entire colum or all rows with missin values. 

```{r}
library(zoo)
data <- na.aggregate(data) #set the average of the column on NA values
```

To check the variability of the data and how they are distributed we made violin plots. As we have a lot of variables the data is split with regards to the column mean in order to make the observation easier.

```{r}
library(tidyr)
library(ggplot2)
library(dplyr)

subset0 <- data.frame(data$x51, data$x52, data$x61, data$x62) # mean less than 2 with high outliers

subset1 <- data.frame(data$x13, data$x14, data$x15, data$x16, data$x17, data$x18, data$x24, data$x25, data$x26, data$x27, data$x28) # mean less than 1

subset1.1 <- data.frame(data$x3, data$x4, data$x19, data$x20, data$x22, data$x23) # mean less than 2

subset2 <- data.frame(data$x12, data$x21, data$x29, data$x30, data$x31, data$x32, data$x33, data$x34, data$x54, data$x60, data$x70) # mean between 2 - 10

subset3 <- data.frame(data$x11, data$x39, data$x40, data$x41, data$x42, data$x43, data$x44, data$x45, data$x46, data$x53, data$x58, data$x59, data$x69, data$x71) # mean between 10 - 50

subset4 <-data.frame(data$x10, data$x37, data$x38, data$x47, data$x48, data$x49, data$x50, data$x68) # mean between 50 - 100

subset5 <- data.frame(data$x1, data$x2, data$x7, data$x8, data$x9, data$x35, data$x36, data$x56, data$x57, data$x63, data$x64, data$x65, data$x66, data$x67, data$x72) # mean between 100 - 1000

subset6 <- data.frame(data$x5, data$x6, data$x55)  # mean higher than 1000


subset0 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means lower than 2 with large outliers")


subset1 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means lower than 1")

subset1.1 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means lower than 2")

subset2 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 2 and 10")

subset3 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 10 and 50")

subset4 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 50 and 100")

subset5 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 100 and 1000")

subset6 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means greater than 1000")


```

Looking at this violin plots it can be seen that a lot of the components have quite small variance. However we can see some high outliers which could be problematic. This is noted when moving on to firther analysis.

A correlation table is done in order to check if there is linear relation between the variables.

```{r}
X <- data[,-73]
X <- scale(X)

C<-cor(X)
head(C)
```

We have some variables that are linearly correlated. For example `x1` and `x2` have a correlation of `r C[1][2]`. 

Next a Cluster Dendrogram is produced in order to check graphically the distances between the variables. It clusters the variables with respect to their column mean.

```{r}
Dvar<-as.dist(2*(1-C)) # as.dist  is a generic function. Its default method handles objects inheriting from class "dist".
clusterVar<-hclust(Dvar,method= "average")
plot(clusterVar,labels=colnames(X),cex=0.65)
```

In this dendrogram it can be seen that there are a lot of couples, a lot of variables which hasn't a lot of distance between them.

##Could we say:

In this dendrogram it can be seen that there are a lot of clusters meaning a lot of the variables are close to each other with respect to the mean.

#Queation: Why is it relevant for us to know that a lot of the parameters has similar means? 

As we have a lot of vairables PCA analysis is preformed in order to check the importance of each varaible.

```{r}
pca <- princomp(data[, 1:72])
summary(pca)
```

Looking to the principal components one can see that with just three components explain almost 95% of the data variation.

The next step is to check the importance that each variable has in each principal component. To simplify this is only checked for the first threee components. 

```{r}
l <- data.frame(pca$loadings[,1:3]) 

names <- row.names(l)

names <- row.names(l)
barplot(l[,1], names.arg = rownames(l), horiz = T)
title("First Principal Component")
barplot(l[,2], names.arg = rownames(l), horiz = T)
title("Second Principal Component")
barplot(l[,3], names.arg = rownames(l), horiz = T)
title("Third Principal Component")
```

In this plots it can be seen that there is a lot of variables that don't contribut much to the principal components. To know which variables are less important we sum the contribution of the variables to the three first principal components and looked at  which ones had a sum less than $0.05$. These arethe ones that can be considered removed if one would want to simplify the analytics. 

```{r}
l$t <- rowSums(l[, 1:3])

barplot(l$t)
title("Contribution of the variables to the first three PC")

l$sign <- l$t < 0.05 # I assume that if the contribution in the sum of the three mean principal components is not higher than 0.05 the variable is not important for data variation.

head(l)

nsign <- which(l$sign == T)
```

We can see that variables **`r nsign`** aren't important for explaining much of the variance of this dataset.

As it's seen there are a lot of variables whom one could consider removing from the original data however we deciside to keep them and use this analysis to compare with the variables that are demed unimportend when we do the prunned tree and the random forest analysisi.

#2 Splitting data

```{r}
data$y=as.factor(data$y)

set.seed(params$seed)
pt <- params$partition

inTrain <- createDataPartition(y=data$y, p=pt, list=FALSE)
str(inTrain)
training <- data[-inTrain,]
testing <- data[inTrain,]
nrow(training)
```


#3 Pruned single tree:

```{r}
tree.data <- tree(y~., data, subset= inTrain, split="deviance")
summary(tree.data)

```

```{r}
set.seed(params$seed)
cv.data=cv.tree(tree.data)
names(cv.data)
cv.data
```

```{r}
prune.data=prune.tree(tree.data,best=3) #this is a local minumim #cv.data$size[which.min(cv.data$dev)])
summary(prune.data)
plot(prune.data)
text(prune.data,pretty=0)
```

```{r}
yhat_1=predict(prune.data,newdata=testing, type = "class")
res <- table(yhat_1,testing$y)
res
accrcy <- sum(diag(res)/sum(res))
accrcy
#sqrd.error.sum <- mean((yhat_1-testing$y)^2)
#sqrd.error.sum
```
The accuracy that we obtain in this model is **`r accrcy`**.

# Random forrest:

create tunegrid with 15 values from 1:15 for mtry to tunning model. Our train function will change number of entry variable at each split according to tunegrid.
```{r}

#Do not run, it takes forever
library(randomForest)
library(mlbench)
library(caret)
library(e1071)

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3, 
                        search='grid')
 
tunegrid <- expand.grid(.mtry = (1:15)) 

rf_gridsearch <- train(y ~ ., 
                       data = training,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid, 
                       trControl=control)
print(rf_gridsearch) #choses the optimum value!! 
```


need to make a loop to tune of ntree with the optimum mtry
```{r}
mtry.opt <- rf.gridsearch$mtry

accuratcy.list <- rep(0, 1000)

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)

for (i in 1:1000){
  model <- train(y ~ ., 
                       data = training,
                       method = 'rf',
                       metric = 'Accuracy',
                       mtry = mtry.opt, 
                       trControl=control)
  yhat <- predict(model, newdata = testing, type = "raw")
  res <- table(yhat,testing$y)
  accrcy.list[i] <- sum(diag(res)/sum(res))
}


```

```{r}
ntree <- which.max(accuratcy.list)

ggplot(data = tibble(x = seq(1, 1000, 1), accuratcy.list), aes(x, accuratcy.list)) + geom_line() + geom_vline(x = ntree)

```
```{r}
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)
 

rf_final <- train(y ~ ., 
                       data = training,
                       method = 'rf',
                       metric = 'Accuracy',
                       ntree = ntree,
                       mtry = mtry.opt,
                       trControl=control)
```

Make predictions with the final model and get accuratcy statistics
```{r}
yhat_final <- predict(rf_final, newdata = testing, type = "raw")
res <- table(yhat_final,testing$y)
res
accrcy <- sum(diag(res)/sum(res))
accrcy
```

Use varImp to get the variable importance. 
```{r}
varImp(rf_final)
```

Need to compare to orriginal results
#5

Just compare the models


#6


#Using gbm function as teacher : Generalized Boosted Regression Modeling
To run gradient boosting algorithm we use GLM funcion.

Concerning the use of the function :
  
- n.trees is equivalent to the number of iterations and the number of basis functions in the additive expansion.

- We begin with trees of 4 nodes to study the iteration influence

Moreover, it is important toreplace binary variables {-1,1} in {0,1} from ou training data set at the begining to use this function.

```{r}
set.seed(2)
#Replace binary variables by {0,1}
training$y <- ifelse(training$y==-1,0,1)

#Fitting the model & predictio
boost.compounds=gbm(y~.,data=training,distribution="adaboost",n.trees=2000,interaction.depth=4)

#Evaluating the error for testing data set
yhat_gbm_2000 =predict(boost.compounds,newdata=testing,n.trees=2000, type = "response")
yhat_gbm_2000 <- ifelse(yhat_gbm_2000 < 0.5,0,1)
res <- table(yhat_gbm_2000,testing$y)
accrcy_gbm_test_2000 <- sum(diag(res)/sum(res))

#Evaluating the error for training data set
yhat_gbm_train_2000 =predict(boost.compounds,newdata=training,n.trees=2000, type = "response")
yhat_gbm_train_2000 <- ifelse(yhat_gbm_train_2000 < 0.5,0,1)
res <- table(yhat_gbm_train_2000,training$y)
accrcy_gbm_train_2000 <- sum(diag(res)/sum(res))
accrcy_gbm_train_2000
```

As we can see, the missclasification rate is really lower on training data set than on testing data set, which seems legit (99,9% againt 78.6%).

Then we can put forward the influence of the number of iterations (ie. the number of trees) on the missclassification on the testing data set:
  
```{r}
sampling = 10
accrcyRates <- rep(0,sampling)
firstValue = 100
lastValue = 2000
nTreeV = floor(seq(from = firstValue, to = lastValue, by = (lastValue - firstValue)/(sampling-1)))

for (i in 1:sampling) {
  boost.compounds=gbm(y~.,data=training,distribution="adaboost",n.trees=nTreeV[i],interaction.depth=4)
  yhat_gbm =predict(boost.compounds,newdata=testing,n.trees=nTreeV[i], type = "response")
  yhat_gbm <- ifelse(yhat_gbm < 0.5,0,1)
  res <- table(yhat_gbm,testing$y)
  err_gbm <- sum(diag(res)/sum(res))
  accrcyRates[i] <- err_gbm
}

plot(nTreeV,accrcyRates)

```  

We can then study the influence of trees maximum depth :
  
```{r}
#We take the n.tree value with the best accrcyRates
nTree <- nTreeV[which(max(accrcyRates)==accrcyRates)]
treeDepth <- c(3,4,8,16)
accrcyRates <- rep(0,length(treeDepth))

k <- 0
for (i in treeDepth) {
  k = k +1
  boost.compounds=gbm(y~.,data=training,distribution="adaboost",n.trees=nTree,interaction.depth=i)
  yhat_gbm =predict(boost.compounds,newdata=testing,n.trees=nTree, type = "response")
  yhat_gbm <- ifelse(yhat_gbm < 0.5,0,1)
  res <- table(yhat_gbm,testing$y)
  err_gbm <- sum(diag(res)/sum(res))
  accrcyRates[k] <- err_gbm
}

plot(treeDepth,accrcyRates)
```



