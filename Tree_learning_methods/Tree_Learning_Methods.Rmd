---
title: "Tree learnign methods"
output: html_document
params:
  show_code: TRUE #possibility to not show the code just the results
  seed: 1234
  author: 'Gregoire Gasparini, Aurora Hofman, Beatriu Tort'
  partition: 0.5
  myDescription: 'The data set contains date on 5631 compound where a solubility screen has been preformed where all compounds were classified as eather soluble of insoluble. Fro each compound 72 contineous variables were recorded. '
  myfile: "soldat.csv"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(caret)
library(tree)
```

#1 Exploratory data analysis

```{r}
data <- read.csv(params$myfile)
```

`r param$myDescription`

A short exploratory data analysis is preformed to become familiar with the dateset.


```{r}
summary(data)
```

In the summary one can see that the only variable that has missing values is $x71$ with a percentage of $13.97$%. To preform analysis one can not have missing values hence we decided to substitute the NA values with the column mean in stead of for example deleting the entire colum or all rows with missin values. 

```{r}
library(zoo)
data <- na.aggregate(data) #set the average of the column on NA values
```

To check the variability of the data and how they are distributed we made violin plots. As we have a lot of variables the data is split with regards to the column mean in order to make the observation easier.

```{r}
library(tidyr)
library(ggplot2)
library(dplyr)

subset0 <- data.frame(data$x51, data$x52, data$x61, data$x62) # mean less than 2 with high outliers

subset1 <- data.frame(data$x13, data$x14, data$x15, data$x16, data$x17, data$x18, data$x24, data$x25, data$x26, data$x27, data$x28) # mean less than 1

subset1.1 <- data.frame(data$x3, data$x4, data$x19, data$x20, data$x22, data$x23) # mean less than 2

subset2 <- data.frame(data$x12, data$x21, data$x29, data$x30, data$x31, data$x32, data$x33, data$x34, data$x54, data$x60, data$x70) # mean between 2 - 10

subset3 <- data.frame(data$x11, data$x39, data$x40, data$x41, data$x42, data$x43, data$x44, data$x45, data$x46, data$x53, data$x58, data$x59, data$x69, data$x71) # mean between 10 - 50

subset4 <-data.frame(data$x10, data$x37, data$x38, data$x47, data$x48, data$x49, data$x50, data$x68) # mean between 50 - 100

subset5 <- data.frame(data$x1, data$x2, data$x7, data$x8, data$x9, data$x35, data$x36, data$x56, data$x57, data$x63, data$x64, data$x65, data$x66, data$x67, data$x72) # mean between 100 - 1000

subset6 <- data.frame(data$x5, data$x6, data$x55)  # mean higher than 1000

subset0 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means lower than 2 with large outliers")


subset1 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means lower than 1")

subset1.1 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means lower than 2")

subset2 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 2 and 10")

subset3 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 10 and 50")

subset4 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 50 and 100")

subset5 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means between 100 and 1000")

subset6 %>% 
  gather(key="compound", value="Val") %>%
  ggplot( aes(x=compound, y=Val, fill=compound)) +
    geom_violin()+
  coord_flip()+
  ggtitle("Plot means greater than 1000")


```

Looking at this violin plots it can be seen that a lot of the components have quite small variance. However we can see some high outliers which could be problematic. This is noted when moving on to firther analysis.

A correlation table is done in order to check if there is linear relation between the variables.

```{r}
X <- data[,-73]
X <- scale(X)

C<-cor(X)
head(C)
```

We have some variables that are linearly correlated. For example `x1` and `x2` have a correlation of `r C[1][2]`. 

Next a Cluster Dendrogram is produced in order to check graphically the distances between the variables. It clusters the variables with respect to their column mean.

```{r}
Dvar<-as.dist(2*(1-C)) # as.dist  is a generic function. Its default method handles objects inheriting from class "dist".
clusterVar<-hclust(Dvar,method= "average")
plot(clusterVar,labels=colnames(X),cex=0.65)
```

In this dendrogram it can be seen that there are a lot of couples, a lot of variables which hasn't a lot of distance between them.

##Could we say:

In this dendrogram it can be seen that there are a lot of clusters meaning a lot of the variables are close to each other with respect to the mean.

#Queation: Why is it relevant for us to know that a lot of the parameters has similar means? 

As we have a lot of vairables PCA analysis is preformed in order to check the importance of each varaible.

```{r}
pca <- princomp(data[, 1:72])
summary(pca)
```

Looking to the principal components one can see that with just three components explain almost 95% of the data variation.

The next step is to check the importance that each variable has in each principal component. To simplify this is only checked for the first threee components. 

```{r}
l <- data.frame(pca$loadings[,1:3]) 

names <- row.names(l)

names <- row.names(l)
barplot(l[,1], names.arg = rownames(l), horiz = T)
title("First Principal Component")
barplot(l[,2], names.arg = rownames(l), horiz = T)
title("Second Principal Component")
barplot(l[,3], names.arg = rownames(l), horiz = T)
title("Third Principal Component")
```

In this plots it can be seen that there is a lot of variables that don't contribut much to the principal components. To know which variables are less important we sum the contribution of the variables to the three first principal components and looked at  which ones had a sum less than $0.05$. These arethe ones that can be considered removed if one would want to simplify the analytics. 

```{r}
l$t <- rowSums(l[, 1:3])

barplot(l$t)
title("Contribution of the variables to the first three PC")

l$sign <- l$t < 0.05 # I assume that if the contribution in the sum of the three mean principal components is not higher than 0.05 the variable is not important for data variation.

head(l)

nsign <- which(l$sign == T)
```

We can see that variables **`r nsign`** aren't important for explaining much of the variance of this dataset.

As it's seen there are a lot of variables whom one could consider removing from the original data however we deciside to keep them and use this analysis to compare with the variables that are demed unimportend when we do the prunned tree and the random forest analysisi.

#2 Splitting data

```{r}
data$y=as.factor(data$y)

set.seed(params$seed)
pt <- params$partition

inTrain <- createDataPartition(y=data$y, p=pt, list=FALSE)
str(inTrain)
training <- data[-inTrain,]
testing <- data[inTrain,]
nrow(training)
```


#3 Pruned single tree:

```{r}
tree.data <- tree(y~., data, subset= inTrain, split="deviance")
summary(tree.data)

```

```{r}
set.seed(params$seed)
cv.data=cv.tree(tree.data)
names(cv.data)
cv.data
```
One can see there is a local minimum at $size = 2$. Since it does not have that much sence to choose the actual mimimum sinse this would not prune the tree at all. 
```{r}
prune.data=prune.tree(tree.data,best=2) #this is a local minumim 
summary(prune.data)
plot(prune.data)
text(prune.data,pretty=0)
```

Make predictions on the test set to evaluate the classifier.
```{r}
yhat_1=predict(prune.data,newdata=testing, type = "class")
res <- table(yhat_1,testing$y)
res
accrcy <- sum(diag(res)/sum(res))

```
The tree has an accuratcy of `r accrcy`. This is not particularly good and we will se later that the random forrest obtains better results. 

# Random forrest:

In the random forrest model one needs to decide how many variables $mtry$ are assesed in each node and how many trees $ntree$ are used in the forrest. Caret only has tuning for the number of parameters assessed in each node and this tuningprocess is quite slow the decition is made to take a gready approch towards tuning these parameters. First we use tuning by caret to tune the number of variables. Then the optimum is chosen and used when tuning the number of trees. The order is decided this way since the number of parameters assesed in each node har a bigger inpack on the result than the number of trees. 

A tunegrid is created with 15 values from 1:15 for $mtry$ to tun the model. Our train function will change number of entry variable at each split according to the tunegrid. The default numebr of trees is $500$. 
```{r}
#Very slow
library(randomForest)
library(mlbench)
library(caret)
library(e1071)

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3, 
                        search='grid')
 
tunegrid <- expand.grid(.mtry = (1:15)) 

rf_gridsearch <- train(y ~ ., 
                       data = training,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid, 
                       trControl=control)
print(rf_gridsearch) 
```
Eventually the optimum value for $mtry$ si chosen automatically by the algoritm. 

A loop is made to tune for $ntree$ with the optimum value for $mtry$. This is also a rather slow process hence the decition is made to only chech for number of trees that are multiples of 50. 
```{r}
mtry.opt <- which.max(rf_gridsearch$results$Accuracy)
accuratcy.list <- rep(0, 20)

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)

for (i in seq(1,1000, by = 50)){
  model <- train(y ~ ., 
                data = training,
                method = 'rf',
                metric = 'Accuracy',
                tuneGrid = data.frame(mtry = mtry.opt),
                ntree = i,
                trControl=control)
  yhat <- predict(model, newdata = testing, type = "raw")
  res <- table(yhat,testing$y)
  accuratcy.list[i] <- sum(diag(res)/sum(res))
  #print(sum(diag(res)/sum(res)))
}


```

As one cna see with the exeption of $ntree= 50 $ there is very little difference in the performance of each itteration. THis supports the assumption that $mtry$ has a greater inpack on the result. 

```{r}
x = seq(1,1000, by = 50)

ntree.opt = which.max(accuratcy.list)
ntree.opt
```


```{r}
ntree <- which.max(accuratcy.list)

ggplot(data = tibble(x = seq(1, 1000, by = 50), accuratcy.list), aes(x, accuratcy.list)) + geom_point() + geom_vline(xintercept =ntree, colour = "blue") +  geom_hline(yintercept = y[which.max(accuratcy.list)], colour = "blue")

```
```{r}
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)
 

rf_final <- train(y ~ ., 
                       data = training,
                       method = 'rf',
                       metric = 'Accuracy',
                       ntree = ntree.opt,
                       tuneGrid = data.frame(mtry = mtry.opt),
                       trControl=control)
```

Make predictions with the final model and get accuratcy statistics
```{r}
yhat_final <- predict(rf_final, newdata = testing, type = "raw")
res <- table(yhat_final,testing$y)
res
accrcy_rf <- sum(diag(res)/sum(res))

```
Now the accuratcy obtained is `r accrcy_rf`. This is much higher thtn with the single tree from earlier. 

Use varImp to get the variable importance, the 10 most important are shown. 
```{r}
varImp(rf_final)
```

Variables originally deamed less important by PCA analytics:
```{r}
nsign
```
As one can see there is some inconsistency here. For example variable $x39$ and $x40$ where deamed not so important by the PCA analysis but are the second and third most important in the random forrest. 

#5 Comparation of models:

The pruend tree had an accuratcy of `r accrcy` while the random forrest has an accuratcy of `r accrcy_rf`. The random forrest preforms significally better. 

Just compare the models


#6 Gradiant Boosting


#Using gbm function as teacher : Generalized Boosted Regression Modeling
To run gradient boosting algorithm we use GLM funcion.

Concerning the use of the function :
  
- n.trees is equivalent to the number of iterations and the number of basis functions in the additive expansion.

- We begin with trees of 4 nodes to study the iteration influence

Moreover, it is important toreplace binary variables {-1,1} in {0,1} from ou training data set at the begining to use this function.

```{r}
set.seed(2)
#Replace binary variables by {0,1}
training$y <- ifelse(training$y==-1,0,1)

#Fitting the model & predictio
boost.compounds=gbm(y~.,data=training,distribution="adaboost",n.trees=2000,interaction.depth=4)

#Evaluating the error for testing data set
yhat_gbm_2000 =predict(boost.compounds,newdata=testing,n.trees=2000, type = "response")
yhat_gbm_2000 <- ifelse(yhat_gbm_2000 < 0.5,0,1)
res <- table(yhat_gbm_2000,testing$y)
accrcy_gbm_test_2000 <- sum(diag(res)/sum(res))

#Evaluating the error for training data set
yhat_gbm_train_2000 =predict(boost.compounds,newdata=training,n.trees=2000, type = "response")
yhat_gbm_train_2000 <- ifelse(yhat_gbm_train_2000 < 0.5,0,1)
res <- table(yhat_gbm_train_2000,training$y)
accrcy_gbm_train_2000 <- sum(diag(res)/sum(res))
accrcy_gbm_train_2000
```

As we can see, the missclasification rate is really lower on training data set than on testing data set, which seems legit (99,9% againt 78.6%).

Then we can put forward the influence of the number of iterations (ie. the number of trees) on the missclassification on the testing data set:
  
```{r}
sampling = 10
accrcyRates <- rep(0,sampling)
firstValue = 100
lastValue = 2000
nTreeV = floor(seq(from = firstValue, to = lastValue, by = (lastValue - firstValue)/(sampling-1)))

for (i in 1:sampling) {
  boost.compounds=gbm(y~.,data=training,distribution="adaboost",n.trees=nTreeV[i],interaction.depth=4)
  yhat_gbm =predict(boost.compounds,newdata=testing,n.trees=nTreeV[i], type = "response")
  yhat_gbm <- ifelse(yhat_gbm < 0.5,0,1)
  res <- table(yhat_gbm,testing$y)
  err_gbm <- sum(diag(res)/sum(res))
  accrcyRates[i] <- err_gbm
}

plot(nTreeV,accrcyRates)

```  

We can then study the influence of trees maximum depth :
  
```{r}
#We take the n.tree value with the best accrcyRates
nTree <- nTreeV[which(max(accrcyRates)==accrcyRates)]
treeDepth <- c(3,4,8,16)
accrcyRates <- rep(0,length(treeDepth))

k <- 0
for (i in treeDepth) {
  k = k +1
  boost.compounds=gbm(y~.,data=training,distribution="adaboost",n.trees=nTree,interaction.depth=i)
  yhat_gbm =predict(boost.compounds,newdata=testing,n.trees=nTree, type = "response")
  yhat_gbm <- ifelse(yhat_gbm < 0.5,0,1)
  res <- table(yhat_gbm,testing$y)
  err_gbm <- sum(diag(res)/sum(res))
  accrcyRates[k] <- err_gbm
}

plot(treeDepth,accrcyRates)
```

```

